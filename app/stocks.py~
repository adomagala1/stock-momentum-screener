import logging
import os
import requests
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from app.save_data import save_stocks_to_csv
from typing import List, Optional


os.makedirs("logs", exist_ok=True)
logging.basicConfig(
    filename="logs/stock_scraper.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/115.0 Safari/537.36"
}

URL_DEFAULT = "https://finviz.com/screener.ashx?v=111"
URL_FILTERED = (
    "https://finviz.com/screener.ashx?v=152&f=cap_mid,exch_nasd,sh_avgvol_o500,"
    "sh_price_o5,sh_relvol_o1.5&c=1,2,3,4,5,6,7,20,42,43,57,58,64,67,65,66"
)

COLUMNS_NORMAL = [
    "No", "Ticker", "Company", "Sector", "Industry", "Country",
    "Market Cap", "P/E", "Price", "Change", "Volume"
]

COLUMNS_FILTERED = [
    "Ticker", "Company", "Sector", "Industry", "Country", "Market Cap", "P/E",
    "EPS next 5Y", "Perf Week", "Perf Month", "52w High", "52w Low",
    "Rel Vol", "Volume", "Price", "Change"
]


def fetch_finviz(
        max_companies: int = 10,
        get_only_tickers: bool = False,
        with_filters: bool = False,
        tickers: Optional[List[str]] = None
) -> pd.DataFrame:
    """
    Pobiera dane z Finviz.
    - Jeśli podano listę `tickers`, pobiera dane tylko dla tych tickerów.
    - W przeciwnym razie działa jako screener z limitem `max_companies`.
    """
    url = URL_FILTERED if with_filters else URL_DEFAULT

    # <-- ZMIANA: Modyfikacja URL, jeśli podano listę tickerów
    if tickers and isinstance(tickers, list):
        ticker_string = ",".join(tickers)
        url += f"&t={ticker_string}"
        # Jeśli szukamy konkretnych tickerów, limit `max_companies` nie ma sensu
        max_companies = len(tickers)

    start_time = datetime.now()
    all_data = []
    start = 1
    companies_fetched = 0
    unlimited = max_companies is None or max_companies <= 0
    columns = ["No", "Ticker"] if get_only_tickers else (COLUMNS_FILTERED if with_filters else COLUMNS_NORMAL)

    while True:
        # Paginacja ma sens tylko wtedy, gdy nie szukamy konkretnych tickerów lub gdy jest ich > 20
        if not unlimited and companies_fetched >= max_companies:
            break

        paged_url = url + f"&r={start}"
        try:
            response = requests.get(paged_url, headers=HEADERS, timeout=10)
            response.raise_for_status()
        except requests.RequestException as e:
            logging.error(f"Błąd połączenia z Finviz: {e}")
            return pd.DataFrame()  # Zwróć pusty DataFrame w przypadku błędu sieciowego

        soup = BeautifulSoup(response.text, "html.parser")
        # Szukamy tabeli z danymi. Finviz ma różne struktury HTML.
        # To jest bardziej odporne podejście.
        screener_table = soup.find("table", {"class": "screener_table"})
        if not screener_table:
            logging.warning("Nie znaleziono tabeli screener_table na stronie.")
            break

        rows = screener_table.find_all("tr", class_=["styled-row-dark", "styled-row-light"])
        if not rows:
            logging.info("Nie znaleziono więcej wierszy z danymi.")
            break

        for row in rows:
            cols = row.find_all("td")
            # Czasem Finviz wstawia puste wiersze lub wiersze reklamowe
            if len(cols) < len(columns):
                continue

            row_data = [col.get_text(strip=True) for col in cols]

            if get_only_tickers:
                all_data.append([row_data[0], row_data[1]])
            else:
                all_data.append(row_data[:len(columns)])

            companies_fetched += 1
            if not unlimited and companies_fetched >= max_companies:
                break

        # Jeśli szukamy konkretnych tickerów, zwykle jest tylko jedna strona wyników
        if tickers:
            break

        start += 20

    if not all_data:
        return pd.DataFrame()

    df = pd.DataFrame(all_data, columns=columns)
    df.fillna(pd.NA, inplace=True)
    finish = datetime.now()
    logging.info(f"Pobrano {len(df)} spółek. Czas: {finish - start_time}")
    return df


def get_current_price(ticker: str) -> float | None:
    """
    Pobiera aktualną cenę danej spółki z Finviz (dostosowane do nowej struktury HTML)
    Zwraca float lub None, jeśli coś pójdzie nie tak.
    """
    try:
        url = f"https://finviz.com/quote.ashx?t={ticker.upper()}"
        response = requests.get(url, headers=HEADERS, timeout=5)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        price_tag = soup.find("strong", class_="quote-price_wrapper_price")
        if price_tag and price_tag.text:
            price_str = price_tag.text.strip().replace("$", "").replace(",", "")
            return float(price_str)

        price_cells = soup.find_all("td", class_="snapshot-td2")
        for cell in price_cells:
            if "$" in cell.text:
                return float(cell.text.replace("$", "").replace(",", ""))

        return None

    except Exception:
        return None


if __name__ == "__main__":
    df = fetch_finviz(max_companies=100, with_filters=False, get_only_tickers=False)
    save_stocks_to_csv(df, with_filters=False, get_only_tickers=False)
